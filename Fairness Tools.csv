No,Paper,Authors,Name,Open-source,Data types ,Metric types,ML task,Mechanism Type,# of sensitive attributes,Description,Usage,Developer,Maturity
1,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",Aequitas,yes,tabular,group,classification,,,"description 1: A toolkit that lets users to test models with regards to several bias and fairness metrics for different population subgroups. Aequitas produces reports from the obtained data that helps data scientists, machine learning researchers, and policymakers to make conscious decisions and avoid harm and damage toward certain populations.","measures bias, audit tool",Carnegie Mellon University,
1,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Aequitas,yes,tabular,group,classification,,,"description 2: An audit toolkit to test modules for other kinds of bias present in population subgroups. In this toolkit, bias detection is done before model selection so that if the data is biased, it can be corrected before training. This toolkit will help data scientists and policymakers avoid harm by making calculated decisions about the AI models.","measures bias, audit tool",Carnegie Mellon University,
1,7. A Review of Bias and Fairness in Artificial Intelligence,"Rubén González-Sendino, Emilio Serrano, Javier Bajo1, Paulo Novais",Aequitas,yes,tabular,group,classification,,,"description 3: Aequitas runs a full report on biases. This report is expected to be used by developers, analysts, and policymakers.","measures bias, audit tool",Carnegie Mellon University,
1,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",Aequitas,yes,tabular,group,classification,,,"description 4: This toolkit generates reports from the obtained data to test if an ML model is fair for diferent subgroups. Aequitas can help people from various professions, such as data scientists, ML researchers, and policymakers.","measures bias, audit tool",Carnegie Mellon University,
1,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",Aequitas,yes,tabular,group,classification,,,description 5: An open-source bias audit toolkit. It mainly focuses on auditing bias in ML data.,"measures bias, audit tool",Carnegie Mellon University,
2,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",AI Fairness 360 (AIF360),yes,tabular,"group, individual","classification, regression","Pre-process, In-process, Post-process",multiple,description 1: A toolkit developed by IBM to help moving fairness research algorithms into an industrial setting and to create a benchmark for fairness algorithms to get evaluated and an environment for fairness researchers to share their ideas.,measures + mitigates bias,IBM,
2,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",AI Fairness 360 (AIF360),yes,tabular,"group, individual","classification, regression","Pre-process, In-process, Post-process",multiple,"description 2: This toolkit is designed to bring together researchers designing the algorithms and data scientists using them in the field. It contains bias detection, mitigation, and explanation algorithms that are ready to use interactively. This toolkit is very helpful, as it provides a platform to experiment and compare different bias detection and mitigation techniques.",measures + mitigates bias,IBM,
2,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",AI Fairness 360 (AIF360),yes,tabular,"group, individual","classification, regression","Pre-process, In-process, Post-process",multiple,"description 3: AI Fairness 360 or AIF360 is an industrial Python toolkit developed by IBM mainly for evaluating fairness algorithms and providing a common framework so that scholars can share their ideas. A complete collection of fairness metrics for datasets and models, justifcations for these metrics, and dataset’s and model’s bias mitigation strategies are included in the package along with an interactive Web experience.",,IBM,
2,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",AI Fairness 360 (AIF360),yes,tabular,"group, individual","classification, regression","Pre-process, In-process, Post-process",multiple,"description 4: A comprehensive extensible open-source fairness library. It integrates the functions of fairness analysis, bias mitigation, and bias metric explanations together.",,IBM,
3,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",COMPAS,,,,,,,A recidivism risk prediction tool (IBM) where prior arrests and friend/family arrests were used as proxy variables to measure level of “riskiness” or “crime”—which on its own can be viewed as mismeasured proxies.,measures bias,,
4,7. A Review of Bias and Fairness in Artificial Intelligence,"Rubén González-Sendino, Emilio Serrano, Javier Bajo1, Paulo Novais",FairLearn,,tabular,group,"classification, regression","Pre-process, In-process, Post-process", multiple,,measures + mitigates bias,,
4,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",FairLearn,,tabular,group,"classification, regression","Pre-process, In-process, Post-process", multiple,A Python package for developers to analyse the fairness of the system and to provide algorithms for mitigating unfairness,mitigates unfairness,Microsoft,
5,7. A Review of Bias and Fairness in Artificial Intelligence,"Rubén González-Sendino, Emilio Serrano, Javier Bajo1, Paulo Novais",WIT,,,,,,,WIT provides a graphical interface in which the behavior of an algorithm can be tested visually. The tool integrates the fairness indicator developed in TensorFlow.,"measures bias, audit tool",,
6,7. A Review of Bias and Fairness in Artificial Intelligence,"Rubén González-Sendino, Emilio Serrano, Javier Bajo1, Paulo Novais",Audit AI,,tabular,group,"classification, regression",,,,,,
6,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",Audit AI,,tabular,group,"classification, regression",,,A toolkit for detecting group unfairness with multiple statistical significance test.,detects group unfairness,Pymetrics team,
7,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",Tensorflow Responsible AI,,,,,,,,,,
8,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",disparate impact remover,,,,,,,"It modifes the data distribution to ensure equal group representation in the training data. The method achieves this by adjusting the weights of each training example based on the protected attribute’s distribution, thus equalizing the acceptance rate across group.",ensures protected group fairness,,
9,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",framework for evaluating the fairness of  prediction models and demonstrating how to apply it to assess the fairness of recidivism prediction instruments ,,,,,,,"This framework has three steps: identifying the group  protected by anti-discrimination laws or ethical considerations, identifying the outcome variable, and evaluating prediction fairness through the “disparate impact"" and “equal opportunity"" tests.",Εvaluates the fairness of  prediction models and demonstrates how to apply it to assess the fairness of recidivism prediction instruments.,,
10,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",What-if,,,,,,,"Interactive visualization tool called the “What-if tool"" to increase awareness of  the potential sources of discrimination in machine learning models. It provides an intuitive and user-friendly interface that allows users to explore the impact of various changes on the fairness and accuracy of machine learning models in real-time and visualize its outputs. Users can change input data points, adjust thresholds, and modify other parameters to observe how diferent decisions afect model performance and fairness through a variety of fairness and  performance metrics.","visualization tool, allows users to explore the impact of various changes on the fairness and accuracy of machine learning models",,
,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",Fairness Indicators,,tabular,group,classification,,,"Fairness Indicators is a library for fairness analysis developed by Google. It is developed based on TensorFlow and provides a visual interface via the What-If Tool. Users can employ Fairness Indicators to analyse the data and trained ML models, and then to visualise the results. However, Fairness Indicators does not provide a function for mitigating bias in ML models.",evaluates fairness of data and measure group fairness on models,Google,
11,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",CF-based solution using a decision-theoretic framework in classifcation,,,,,,,CF-based solution using a decision-theoretic framework in classifcation. Their proposed framework extends the standard  cost-sensitive learning approach by introducing a social cost matrix to capture the societal costs associated with diferent  types of errors. The model also includes a two-step process for building a discrimination-aware classifer.,eliminate risk assessment instruments' (RAI) bias,,
12,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",DiCE,yes,,,,,,Quantitative evaluation framework for counterfactuals that allows fne-tuning for a particular scenario and enables comparison between CF-based and other local explanation based methods. This tool provides diverse counterfactual instances that are diferent from the original but represent the same class. This method used diversity metrics and proximity constraints for generating diverse and feasible CFs (where the prediction is binary).,allows fne-tuning for a particular scenario and enables comparison between CF-based and other local explanation based methods,,
13,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",ViCE,,,,,,,"Black-box visual analytic tool that enhances the interpretability of machine learning models in the context of visual tasks. The approach focuses on generating counterfactual explanations by providing alternative visual examples that would lead to diferent model predictions. It uses a heuristic search algorithm, a Gaussian technique in features, and a greedy approach to discover the lowest set of viable adjustments for changing the outcome.",visual analytic tool that enhances the interpretability,,
14,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",CERTIFAI,,tabular,group,"classification, regression",,,"description 1: The development of counterfactual explanations is the main focus of CERTIFAI (Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artifcial Intelligence models). Existing counterfactual generating algorithms have a few shortcomings, such as infeasible examples. It hinders the process of evaluating the robustness and fairness of developed models. This tool aims to provide a solution to this issue",evaluates the robustness and fairness of developed models,CognitiveScale,
14,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",CERTIFAI,,tabular,group,"classification, regression",,,description 2: A library includes several metrics for evaluating fairness in trained models.,evaluates fairness in trained models,CognitiveScale,
15,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",FairSight,,tabular,"group, individual",ranking,"Pre-process, In-process, Post-process",single,"A platform for measuring, diagnosing, and mitigating bias in data and ML models by four categories of fairness metrics and three categories of bias mitigation algorithms.","measures, diagnoses + mitigates bias",University of Pittsburgh,
16,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",Responsibly,,"tabular, text",group,classification,"Pre-process, In-process, Post-process",multiple,A Python toolkit for auditing bias and fairness of ML systems and mitigating bias through algorithmic interventions.,audits + mitigates bias and fairness,Boston University,
17,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",Fairness,,tabular,group,classification,"Pre-process, In-process, Post-process",multiple,An R package for evaluating ML models’ group fairness.,evaluates models' group fairness,Nikita Kozodoi and Tibor V. Varga,
18,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",FairTest,,tabular,group,"classification, regression",,,A toolkit for discovering unwarranted associations between the outputs of an algorithm and certain groups by sensitive attributes.,discovers unwarranted associations between the outputs of an algorithm and certain groups,Stanford and Columbia University,
19,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",Fairness Measures,,tabular,group,classification,,,A toolkit for measuring the fairness on data.,measures fairness,TU Berlin,
20,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",ML Fairness Gym,,tabular,"group, individual","dynamic environment, classification",,,A toolkit for analysing the fairness of algorithms in dynamical systems.,analyses the fairness of algorithms in dynamical systems,Google,
21,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",Algofairness,,tabular,group,classification,"Pre-process, In-process","multiple",A library includes a set of benchmarks for comparing the performance of ML algorithms in fairness and accuracy.,compares the performance of ML algorithms in fairness and accuracy,Haverford,
22,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",GD-IQ,,video,group,bias detection,,,Measure screen and speaking time to determine gender bias in videos.,Measure screen and speaking time to determine gender bias in videos,Mount Saint Mary’s University,
23,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",VERB,,text,individual,NLP,In-process,single,A tool allows users to visualise the embedded vectorised words and the process of mitigating gender bias.,visualises the embedded vectorised words and the process of mitigating gender bias,University of Utah,
24,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",AllenNLP,,text,individual,NLP,In-process,single,A library integrates multiple bias mitigation algorithms on language models.,integrates multiple bias mitigation algorithms on language models,AllenNLP team,
25,"10. FairerML: An Extensible Platform for Analysing, Visualising, and Mitigating Biases in Machine Learning","Bo Yuan, Shenhao Gui, Qingquan Zhang, and Ziqi Wang",FairerML,,tabular,"group, individual",classification,In-process,multiple,"An extensible platform for analysing, visualising, and mitigating biases in ML.","analyses, visualises + mitigates biases in ML",,