No,Paper,Authors,Name,Size,Area/Domain,Open-source,Bias Cases/Protected Attributes,Bias types ,Mechanism Type,Base AI Algorithm,Fairness metrics,Model/Method,Dataset Description,Model/Method Description
1,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",ImageNet,,,,,"Data, Representation",,,,,,
1,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",ImageNet,,,,,"Data, Representation","In-process, Post-process",Any score-based,"Demographic parity, Equalized odds",Decoupled Classifiers,,Decoupling technique to learn a different classifier for each group and then use transfer learning technique to learn from out-of group samples.
2,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",Open Images,,,,,"Data, Representation",,,,,,
3,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",23andMe,,,,Genotype,,,,,,,
4,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",UK Biobank,,Genetic,,,,,,,,,
5,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",UCI Adult,"48,842 income records",Financial,yes,,Representation,,,,,"description 1: Contains information, extracted from the 1994 census data about people with attributes such as age, occupation, education, race, sex, marital status, native country, hours per week, and so on, indicating whether the income of a person exceeds $50K/yr or not. It can be used in fairness-related studies that want to compare gender or race inequalities based on people’s annual incomes, or various other studies.",
5,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult,"48,842 income records",Financial,yes,"Gender/Sex, Race, Age",Representation,Pre-process,Any,Disparate impact,Removing Disparate Impact,"description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.",Pre-process data to decrease earth moving distance between distributions of both groups. Note that it does not require any ground truth data.
5,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult,"48,842 income records",Financial,yes,"Gender/Sex, Race, Age",Representation,Pre-process,Any score based,Demographic parity,"Massaging, Reweighing, Sampling, Suppression","description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.","Massaging: Changing some labels before training (selecting points with lower acceptance rate, trying to replace a small number of samples as opposed to the work of Luong et al.). Reweighing: Assigning different weights to samples. Sampling: Uniform sampling or preferential sampling, using the idea that the closest the samples are to the boundary, the more likely they are to be discriminated. Suppression: Baseline approach to remove correlated features (performed worse)."
5,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult,"48,842 income records",Financial,yes,"Gender/Sex, Race, Age",Representation,Pre-process,Any,Conditional statistical parity,Discrimination Prevention with KNN,"description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.","Changing labels before training such that individuals that are similar except their sensitive attribute will not have different labels, and more individuals from the unprivileged group will get positive labels."
5,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult,"48,842 income records",Financial,yes,"Gender/Sex, Race, Age",Representation,Pre-process,Any,Disparate impact,Optimized Pre-Processing for Discrimination Prevention,"description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.",Transforming the data to a new mapping that enhances both group fairness and individual fairness while maintaining utility. An optimization formulation that incorporates these three requirements.
5,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult,"48,842 income records",Financial,yes,"Gender/Sex, Race, Age",Representation,Pre-process,Any,Group preference-based fairness,Decoupled Classifiers with Preference Guarantees,"description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.",Recursive feature selection procedure for building decoupled classifiers.
5,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult,"48,842 income records",Financial,yes,"Gender/Sex, Race, Age",Representation,In-process,Decision boundary based,Equalized odds,Penalizing Unfairness,"description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.",Minimizing a proxy for equalized odds in a penalty to the objective function.
5,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult,"48,842 income records",Financial,yes,"Gender/Sex, Race, Age",Representation,In-process,SVM,Disparate impact,Dataset Constraints,"description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.","Constrained non-convex optimization using SVM with ramp loss, to enhance disparate impact."
5,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult,"48,842 income records",Financial,yes,"Gender/Sex, Race, Age",Representation,In-process,Convex boundary-based classifiers,Group preference-based fairness,Achieving preferred treatment and preferred impact,"description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.",In-process mechanisms that solves an optimization problem that balances accuracy and tractable proxies for preferred treatment and preferred impact.
5,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult,"48,842 income records",Financial,yes,"Gender/Sex, Race, Age",Representation,In-process,SVM,"Equalized odds, Overall accuracy equality",Recycling Privileged Learning and Distribution Matching,"description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.",Use SVM with privileged learning. Combine the sensitive attributes as privileged information that is known only at training time. Use the MMD criterion to encourage the distributions to be similar across privileged and unprivileged groups.
5,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult,"48,842 income records",Financial,yes,"Gender/Sex, Race, Age",Representation,In-process,Any,"Demographic parity, Equalized odds",Reductions Approach,"description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.",A minimax optimization formulation that aims at maximizing the predictor’s capability to predict the outcome while minimizing the capability to predict the sensitive feature. Solved using saddle point methods.
5,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult,"48,842 income records",Financial,yes,"Gender/Sex, Race, Age",Representation,Post-process,Any,Combination of individual and group fairness,Individual+Group Debiasing (IGD),"description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.",Post-process mechanism to balance accuracy with both individual and group fairness. Begin by detecting samples that are prone to individual bias and consider them for prediction change for enhancing disparate impact. Note that this method does not require ground truth class labels for the validation set.
5,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult,"48,842 income records",Financial,yes,"Gender/Sex, Race, Age",Representation,"In-process, Post-process","In-process - decision tree, Post-process - any algorithm",Demographic parity,"Discrimination Aware Tree Construction (new split criterion), Relabeling (post-process)","description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.","New split criterion (in-process) while maximizing information gain between selected split attribute to class label, also minimizing information gain to sensitive attribute. Leaf relabeling (post-process)—change class labels of some leaves trading as little accuracy as possible, using KNAPSACK optimization formulation."
5,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult,"48,842 income records",Financial,yes,"Gender/Sex, Race, Age",Representation,"Pre-process, In-process",Logistic regression,Demographic parity,Learning Fair Representations,"description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.","Combining fair representation learning with an in-process model by a multi-objective loss function that requires representations to retain information of the original vectors, induce fair statistical parity, and maintain as high accuracy as possible."
5,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult,"48,842 income records",Financial,yes,"Gender/Sex, Race, Age",Representation,"Pre-process, In-process",Any,"Demographic parity, Mean difference",Variational Fair Autoencoder,"description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.",Fair representation learning using a variational autoencoder. Additional penalty for controlling distribution matching between groups using MMD criterion.
5,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",UCI Adult,"48,842 income records",Financial,yes,"Gender/Sex, Race, Age",Representation,,,,,"description 3: The UCI Adult dataset, also known as the Census Income dataset, is a popular dataset used in machine learning and data mining research. It consists of 14 attributes or features. These attributes capture various demographic, social, and economic information about individuals. The type of attributes includes a mix of categorical and numerical attributes. Categorical attributes represent specifc characteristics such as education, marital status, occupation, and relationship status. Numerical attributes include age, educational years, capital gain, loss, and weekly work hours. The predicting attribute in the UCI Adult dataset is typically the “income” attribute, which indicates whether an individual earns more than 50, 000 per year. This attribute serves as a binary label, often used for classifcation tasks. The UCI Adult dataset is valued for its real-world relevance, the presence of socio-economic attributes, and the opportunities it provides for studying fairness, bias, and income prediction tasks. Its availability and well-documented nature make it a suitable dataset for many feld researchers",
5,11. A survey on datasets for fairness-aware machine learning,"Tai Le Quy1, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, Eirini Ntoutsi",UCI Adult,"48,842 income records",Financial,,"Gender/Sex, Race, Age",Representation,,,,,"description 4: The classification task is to decide whether the annual income of a person exceeds 50,000 US dollars based on demographic characteristics. The class attribute is income ≤50K, > 50K indicating whether an individual makes less or more than 50K.",
,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult (test only),,Income,yes,"Age, Gender/Sex",Representation,In-process,Logistic regression,Normalized prejudice index Prejudice,Prejudice Remover Regularizer,"description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.",Solving an optimization problem with a penalty that minimizes mutual information between prediction and sensitive attribute.
,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",UCI Adult (test only),,Income,yes,"Age, Gender/Sex",Representation,"In-process, Post-process",Naive Bayes,Demographic parity,"Modifying Naive Bayes, Two Naive Bayes, Expectation Maximization (EM)","description 2: The Adult dataset is a publicly available dataset in the UCI repository based on 1994 U.S. census data. The goal of this dataset is to successfully predict whether an individual earns more or less than $50,000 per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset include age, gender, and race.","Modifying naive Bayes—modify probabilities so that the unprivileged group would get more positive predictions, enhancing demographic parity. Two naive Bayes—using two separate models to predict each of the groups, and balance demographic parity of both groups in an iterative process, by making slight changes to the observed probabilities. EM—add a latent variable, which represents actual labels, to the Bayesian model and optimize using EM."
6,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",imSitu,,,,,,,,,RBA (reducing bias amplification),,
7,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",labeled faces in the wild (LFW),,,,,,,,,Fair PCA,,
8,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",Europarl,,,,Gender/Sex,,,,,,"large political, multilingual dataset used in machine translation",
9,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",German credit,"1,000 credit records",Financial,yes,,,,,,,"description 1: The German Credit dataset contains 1,000 credit records containing attributes such as personal status and sex, credit score, credit amount, housing status and so on. It can be used in studies about gender inequalities on credit-related issues.",
9,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",German credit,"1,000 credit records",Financial,yes,"Age, Gender/Sex",,Pre-process,Any,Disparate impact,Removing Disparate Impact,"description 2: The German dataset is a publicly available dataset in the UCI repository that includes information of individuals from a German bank in 1994. The goal of this dataset is to predict whether an individual should receive a good or bad credit risk score based on features such as employment, housing, savings, and age. The sensitive attributes in this dataset include gender  and age. This dataset is significantly smaller, with only 1,000 individuals with 20 attributes.",Pre-process data to decrease earth moving distance between distributions of both groups. Note that it does not require any ground truth data.
9,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",German credit,"1,000 credit records",Financial,yes,"Age, Gender/Sex",,Pre-process,Any score based,Demographic parity,"Massaging, Reweighing, Sampling, Suppression","description 2: The German dataset is a publicly available dataset in the UCI repository that includes information of individuals from a German bank in 1994. The goal of this dataset is to predict whether an individual should receive a good or bad credit risk score based on features such as employment, housing, savings, and age. The sensitive attributes in this dataset include gender  and age. This dataset is significantly smaller, with only 1,000 individuals with 20 attributes.","Massaging: Changing some labels before training (selecting points with lower acceptance rate, trying to replace a small number of samples as opposed to the work of Luong et al.). Reweighing: Assigning different weights to samples. Sampling: Uniform sampling or preferential sampling, using the idea that the closest the samples are to the boundary, the more likely they are to be discriminated. Suppression: Baseline approach to remove correlated features (performed worse)."
9,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",German credit,"1,000 credit records",Financial,yes,"Age, Gender/Sex",,Pre-process,Any,Conditional statistical parity,Discrimination Prevention with KNN,"description 2: The German dataset is a publicly available dataset in the UCI repository that includes information of individuals from a German bank in 1994. The goal of this dataset is to predict whether an individual should receive a good or bad credit risk score based on features such as employment, housing, savings, and age. The sensitive attributes in this dataset include gender  and age. This dataset is significantly smaller, with only 1,000 individuals with 20 attributes.","Changing labels before training such that individuals that are similar except their sensitive attribute will not have different labels, and more individuals from the unprivileged group will get positive labels."
9,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",German credit,"1,000 credit records",Financial,yes,"Age, Gender/Sex",,Post-process,Any,Combination of individual and group fairness,Individual+Group Debiasing (IGD),"description 2: The German dataset is a publicly available dataset in the UCI repository that includes information of individuals from a German bank in 1994. The goal of this dataset is to predict whether an individual should receive a good or bad credit risk score based on features such as employment, housing, savings, and age. The sensitive attributes in this dataset include gender  and age. This dataset is significantly smaller, with only 1,000 individuals with 20 attributes.",Post-process mechanism to balance accuracy with both individual and group fairness. Begin by detecting samples that are prone to individual bias and consider them for prediction change for enhancing disparate impact. Note that this method does not require ground truth class labels for the validation set.
9,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",German credit,"1,000 credit records",Financial,yes,"Age, Gender/Sex",,"Pre-process, In-process",Logistic regression,Demographic parity,Learning Fair Representations,"description 2: The German dataset is a publicly available dataset in the UCI repository that includes information of individuals from a German bank in 1994. The goal of this dataset is to predict whether an individual should receive a good or bad credit risk score based on features such as employment, housing, savings, and age. The sensitive attributes in this dataset include gender  and age. This dataset is significantly smaller, with only 1,000 individuals with 20 attributes.","Combining fair representation learning with an in-process model by a multi-objective loss function that requires representations to retain information of the original vectors, induce fair statistical parity, and maintain as high accuracy as possible."
9,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",German credit,"1,000 credit records",Financial,yes,"Age, Gender/Sex",,"Pre-process, In-process",Any,"Demographic parity, Mean difference",Variational Fair Autoencoder,"description 2: The German dataset is a publicly available dataset in the UCI repository that includes information of individuals from a German bank in 1994. The goal of this dataset is to predict whether an individual should receive a good or bad credit risk score based on features such as employment, housing, savings, and age. The sensitive attributes in this dataset include gender  and age. This dataset is significantly smaller, with only 1,000 individuals with 20 attributes.",Fair representation learning using a variational autoencoder. Additional penalty for controlling distribution matching between groups using MMD criterion.
9,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",German credit,"1,000 credit records",Financial,yes,"Age, Gender/Sex",,In-process,,"Absolute Balanced Accuracy Difference, Absolute Average Odds Difference, Absolute Equal Opportunity Rate Difference, Statistical Parity Difference",,"description 3: The German dataset has 1000 items and 20 categorical attributes. Each entry in this dataset represents an individual who receives credit from a bank. According to the set of attributes, each individual is evaluated on his or her credit risk.",
9,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",German credit,"1,000 credit records",Financial,yes,"Age, Gender/Sex",,Post-process,,Disparate impact,,"description 3: The German dataset has 1000 items and 20 categorical attributes. Each entry in this dataset represents an individual who receives credit from a bank. According to the set of attributes, each individual is evaluated on his or her credit risk.",
9,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",German credit,"1,000 credit records",Financial,yes,"Age, Gender/Sex",,,,,,"desctiption 4: It is a well-known dataset used in machine learning and credit risk analysis. It consists of 20 attributes or features (a mix of categorical and numerical attributes) that capture various aspects of individuals applying for credit, including personal, fnancial, and employment information. Categorical attributes include sex, housing status, employment type, and credit history. Numerical attributes include features like age, credit amount, duration of credit, and installment rate. The predicting attribute in the UCI German Credit Dataset is typically the “credit risk” attribute, which indicates whether a person is considered a good or bad credit risk. This binary label is used for classifcation tasks to predict the creditworthiness of applicants.",
9,11. A survey on datasets for fairness-aware machine learning,"Tai Le Quy1, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, Eirini Ntoutsi",German credit,"1,000 credit records",Financial,yes,"Age, Gender/Sex",,,,,,"description 5: The German credit10 dataset consists of samples of bank account holders. The dataset is used for risk assessment prediction, that is, to determine whether it is risky to grant credit to a person or not. The dataset is frequently employed in fairness-aware learning researches.",
10,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",Pilot parliaments benchmark,"1,270 images",Facial images,,,,,,,,"The Pilot Parliaments Benchmark dataset, also known as PPB, contains images of 1,270 individuals in the national parliaments from three European (Iceland, Finland, Sweden) and three African (Rwanda, Senegal, South Africa) countries. This benchmark was released to have more gender and race balance, diversity, and representativeness.",
11,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",WinoBias,"3,160 sentences",Coreference resolution,,Coreference resolution,,,,,,"The WinoBias dataset follows the winograd format and has 40 occupations in sentences that are referenced to human pronouns. There are two types of challenge sentences in the dataset requiring linkage of gendered pronouns to either male or female stereotypical occupations. It was used in the coreference resolution study to certify if a system has gender bias or not—in this case, towards stereotypical occupations.",
12,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",Communities and crime,"1,994 crime records",Criminology,yes,,,,,,,"description 1: The Communities and Crime dataset gathers information from different communities in the United States related to several factors that can highly influence some common crimes such as robberies, murders, or rapes. The data includes crime data obtained from the 1990 US LEMAS survey and the 1995 FBI Unified Crime Report. It also contains socio-economic data from the 1990 US Census.",
12,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Communities and crime,"1,994 crime records",Criminology,yes,Percentage of African-American population,,Pre-process,Any score based,Demographic parity,"Massaging, Reweighing, Sampling, Suppression","description 2: The Communities and Crimes dataset includes 1,994 instances and 128 attributes of communities in the United States. It is publicly available in the UCI repository. The goal is to predict the number of violent crimes per 100,000 individuals based on features such as percentage of population by age, by marital status, by number of children, by race, and more. Kamiran and Calders add a new sensitive attribute that represents whether the percentage of the African-American population in the community is greater than 0.06.","Massaging: Changing some labels before training (selecting points with lower acceptance rate, trying to replace a small number of samples as opposed to the work of Luong et al.). Reweighing: Assigning different weights to samples. Sampling: Uniform sampling or preferential sampling, using the idea that the closest the samples are to the boundary, the more likely they are to be discriminated. Suppression: Baseline approach to remove correlated features (performed worse)."
12,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Communities and crime,"1,994 crime records",Criminology,yes,Percentage of African-American population,,Pre-process,Any,Conditional statistical parity,Discrimination Prevention with KNN,"description 2: The Communities and Crimes dataset includes 1,994 instances and 128 attributes of communities in the United States. It is publicly available in the UCI repository. The goal is to predict the number of violent crimes per 100,000 individuals based on features such as percentage of population by age, by marital status, by number of children, by race, and more. Kamiran and Calders add a new sensitive attribute that represents whether the percentage of the African-American population in the community is greater than 0.06.","Changing labels before training such that individuals that are similar except their sensitive attribute will not have different labels, and more individuals from the unprivileged group will get positive labels."
12,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Communities and crime,"1,994 crime records",Criminology,yes,Percentage of African-American population,,Pre-process,Any,Pairwise fairness,Pairwise Fair Representation,"description 2: The Communities and Crimes dataset includes 1,994 instances and 128 attributes of communities in the United States. It is publicly available in the UCI repository. The goal is to predict the number of violent crimes per 100,000 individuals based on features such as percentage of population by age, by marital status, by number of children, by race, and more. Kamiran and Calders add a new sensitive attribute that represents whether the percentage of the African-American population in the community is greater than 0.06.",Pre-process mechanism to create fair representations by leveraging external knowledge on human judgments on pairs of individuals that should be treated similarly. Use fairness graph constraints combined with data-driven similarities.
12,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Communities and crime,"1,994 crime records",Criminology,yes,Percentage of African-American population,,"In-process, Post-process","In-process - decision tree, Post-process - any algorithm",Demographic parity,"Discrimination Aware Tree Construction (new split criterion), Relabeling (post-process)","description 2: The Communities and Crimes dataset includes 1,994 instances and 128 attributes of communities in the United States. It is publicly available in the UCI repository. The goal is to predict the number of violent crimes per 100,000 individuals based on features such as percentage of population by age, by marital status, by number of children, by race, and more. Kamiran and Calders add a new sensitive attribute that represents whether the percentage of the African-American population in the community is greater than 0.06.","New split criterion (in-process) while maximizing information gain between selected split attribute to class label, also minimizing information gain to sensitive attribute. Leaf relabeling (post-process)—change class labels of some leaves trading as little accuracy as possible, using KNAPSACK optimization formulation."
12,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",Communities and crime,"1,994 crime records",Criminology,yes,Percentage of African-American population,,Post-process,,Disparate impact,,"description 3: The Communities dataset compares socioeconomic situations of US citizens in the 1990s and crime rate, identifying the per capita rate of violent crime in each community.",
12,11. A survey on datasets for fairness-aware machine learning,"Tai Le Quy1, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, Eirini Ntoutsi",Communities and crime,"1,994 crime records",Criminology,yes,Percentage of African-American population,,,,,,"description 4: The Communities and Crime dataset was a small dataset containing the socioeconomic data from 46 states of the United States in 1990 (the US Census). The law enforcement data come from the 1990 US LEMAS survey, and crime data come from the 1995 FBI Uniform Crime Reporting program. The goal is to predict the total number of violent crimes per 100 thousand population.",
13,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",Recidivism in juvenile justice,"4,753 crime records",Social,,,,,,,,The Recidivism in Juvenile Justice dataset contains all juvenile offenders between ages 12–17 who committed a crime between years 2002 and 2010 and completed a prison sentence in 2010 in Catalonia’s juvenile justice system.,
14,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Dutch Census,"189,725",Financial,yes,Gender/Sex,,Pre-process,Any score based,Demographic parity,"Massaging, Reweighing, Sampling, Suppression","description 1: The Dutch Census dataset includes 189,725 instances and 13 attributes of individuals. It is publicly available in the IPUMS repository. Kamiran and Calders and Agarwal et al. use this dataset with only the 60,420 individuals who are not underaged. Their goal is to predict whether an individual holds a highly prestigious occupation by using features such as gender, age, household details, location, citizenship, birth country, education, economic status, and marital status. The sensitive feature utilized is gender.","Massaging: Changing some labels before training (selecting points with lower acceptance rate, trying to replace a small number of samples as opposed to the work of Luong et al.). Reweighing: Assigning different weights to samples. Sampling: Uniform sampling or preferential sampling, using the idea that the closest the samples are to the boundary, the more likely they are to be discriminated. Suppression: Baseline approach to remove correlated features (performed worse)."
14,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Dutch Census,"189,725",Financial,yes,Gender/Sex,,In-process,Any,"Demographic parity, Equalized odds",Reductions Approach,"description 1: The Dutch Census dataset includes 189,725 instances and 13 attributes of individuals. It is publicly available in the IPUMS repository. Kamiran and Calders and Agarwal et al. use this dataset with only the 60,420 individuals who are not underaged. Their goal is to predict whether an individual holds a highly prestigious occupation by using features such as gender, age, household details, location, citizenship, birth country, education, economic status, and marital status. The sensitive feature utilized is gender.",A minimax optimization formulation. that aims at maximizing the predictor’s capability to predict the outcome while minimizing the capability to predict the sensitive feature. Solved using saddle point methods.
14,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Dutch Census,"189,725",Financial,yes,Gender/Sex,,"In-process, Post-process","In-process - decision tree, Post-process - any algorithm",Demographic parity,"Discrimination Aware Tree Construction (new split criterion), Relabeling (post-process)","description 1: The Dutch Census dataset includes 189,725 instances and 13 attributes of individuals. It is publicly available in the IPUMS repository. Kamiran and Calders and Agarwal et al. use this dataset with only the 60,420 individuals who are not underaged. Their goal is to predict whether an individual holds a highly prestigious occupation by using features such as gender, age, household details, location, citizenship, birth country, education, economic status, and marital status. The sensitive feature utilized is gender.","New split criterion (in-process) while maximizing information gain between selected split attribute to class label, also minimizing information gain to sensitive attribute. Leaf relabeling (post-process)—change class labels of some leaves trading as little accuracy as possible, using KNAPSACK optimization formulation."
14,11. A survey on datasets for fairness-aware machine learning,"Tai Le Quy1, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, Eirini Ntoutsi",Dutch Census,"189,725",Financial,yes,Gender/Sex,,,,,,description 2: The Dutch census dataset represented aggregated groups of people in the Netherlands for the year 2001. Researchers have used Dutch dataset to formulate a binary classification task to predict a person's occupation which can be categorized as high-level (prestigious) or low-level profession.,
15,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",Diversity in faces,"1,000,000 images",Facial images,,,,,,,,"The Diversity in Faces (DiF) is an image dataset collected for fairness research in face recognition. DiF is a large dataset containing one million annotations for face images. It is also a diverse dataset with diverse facial features, such as different craniofacial distances, skin color, facial symmetry and contrast, age, pose, gender, resolution, along with diverse areas and ratios.",
16,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",ProPublica,"6,167",Criminal risk,yes,"Race, Gender/Sex",,Pre-process,Any,Disparate impact,Optimized Pre-Processing for Discrimination Prevention,"The ProPublica dataset includes data from the COMPAS risk assessment system. This dataset was previously extensively used for fairness analysis in the field of criminal justice risk. The dataset includes 6,167 individuals, and the features in the dataset include number of previous felonies, charge degree, age, race, and gender. The target variable indicates whether an inmate recidivated (was arrested again) within 2 years after release from prison. As for the sensitive variable, this dataset was previously used with two variations: the first when race was considered as the sensitive attribute and the second when gender was considered as the sensitive attribute. The dataset is publicly available from Larson et al.",Transforming the data to a new mapping that enhances both group fairness and individual fairness while maintaining utility. An optimization formulation that incorporates these three requirements.
16,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",ProPublica,"6,167",Criminal risk,yes,"Race, Gender/Sex",,Pre-process,Any,Group preference-based fairness,Decoupled Classifiers with Preference Guarantees,"The ProPublica dataset includes data from the COMPAS risk assessment system. This dataset was previously extensively used for fairness analysis in the field of criminal justice risk. The dataset includes 6,167 individuals, and the features in the dataset include number of previous felonies, charge degree, age, race, and gender. The target variable indicates whether an inmate recidivated (was arrested again) within 2 years after release from prison. As for the sensitive variable, this dataset was previously used with two variations: the first when race was considered as the sensitive attribute and the second when gender was considered as the sensitive attribute. The dataset is publicly available from Larson et al.",Recursive feature selection procedure for building decoupled classifiers.
16,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",ProPublica,"6,167",Criminal risk,yes,"Race, Gender/Sex",,Pre-process,Any,Pairwise fairness,Pairwise Fair Representation,"The ProPublica dataset includes data from the COMPAS risk assessment system. This dataset was previously extensively used for fairness analysis in the field of criminal justice risk. The dataset includes 6,167 individuals, and the features in the dataset include number of previous felonies, charge degree, age, race, and gender. The target variable indicates whether an inmate recidivated (was arrested again) within 2 years after release from prison. As for the sensitive variable, this dataset was previously used with two variations: the first when race was considered as the sensitive attribute and the second when gender was considered as the sensitive attribute. The dataset is publicly available from Larson et al.",Pre-process mechanism to create fair representations by leveraging external knowledge on human judgments on pairs of individuals that should be treated similarly. Use fairness graph constraints combined with data-driven similarities.
16,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",ProPublica,"6,167",Criminal risk,yes,"Race, Gender/Sex",,In-process,Decision boundary based,Disparate impact,Fairness Constraints,"The ProPublica dataset includes data from the COMPAS risk assessment system. This dataset was previously extensively used for fairness analysis in the field of criminal justice risk. The dataset includes 6,167 individuals, and the features in the dataset include number of previous felonies, charge degree, age, race, and gender. The target variable indicates whether an inmate recidivated (was arrested again) within 2 years after release from prison. As for the sensitive variable, this dataset was previously used with two variations: the first when race was considered as the sensitive attribute and the second when gender was considered as the sensitive attribute. The dataset is publicly available from Larson et al.",Optimization using additional constraints to control the covariance.
16,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",ProPublica,"6,167",Criminal risk,yes,"Race, Gender/Sex",,In-process,Decision boundary based,Equalized odds,Removing Disparate Mistreatment,"The ProPublica dataset includes data from the COMPAS risk assessment system. This dataset was previously extensively used for fairness analysis in the field of criminal justice risk. The dataset includes 6,167 individuals, and the features in the dataset include number of previous felonies, charge degree, age, race, and gender. The target variable indicates whether an inmate recidivated (was arrested again) within 2 years after release from prison. As for the sensitive variable, this dataset was previously used with two variations: the first when race was considered as the sensitive attribute and the second when gender was considered as the sensitive attribute. The dataset is publicly available from Larson et al.",Minimizing a proxy for equalized odds with constraints.
16,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",ProPublica,"6,167",Criminal risk,yes,"Race, Gender/Sex",,In-process,Decision boundary based,Equalized odds,Penalizing Unfairness,"The ProPublica dataset includes data from the COMPAS risk assessment system. This dataset was previously extensively used for fairness analysis in the field of criminal justice risk. The dataset includes 6,167 individuals, and the features in the dataset include number of previous felonies, charge degree, age, race, and gender. The target variable indicates whether an inmate recidivated (was arrested again) within 2 years after release from prison. As for the sensitive variable, this dataset was previously used with two variations: the first when race was considered as the sensitive attribute and the second when gender was considered as the sensitive attribute. The dataset is publicly available from Larson et al.",Minimizing a proxy for equalized odds in a penalty to the objective function.
16,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",ProPublica,"6,167",Criminal risk,yes,"Race, Gender/Sex",,In-process,Logistic regression,Procedural fairness,Feature Selection for Procedurally Fair Learning,"The ProPublica dataset includes data from the COMPAS risk assessment system. This dataset was previously extensively used for fairness analysis in the field of criminal justice risk. The dataset includes 6,167 individuals, and the features in the dataset include number of previous felonies, charge degree, age, race, and gender. The target variable indicates whether an inmate recidivated (was arrested again) within 2 years after release from prison. As for the sensitive variable, this dataset was previously used with two variations: the first when race was considered as the sensitive attribute and the second when gender was considered as the sensitive attribute. The dataset is publicly available from Larson et al.","Perform fair feature selection to balance procedural fairness (that considers fair process rather than fair outcome) and accuracy, using constrained submodular optimization. Combines human judgments to assess procedural fairness."
16,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",ProPublica,"6,167",Criminal risk,yes,"Race, Gender/Sex",,In-process,Convex boundary-based classifiers,Group preference-based fairness,Achieving preferred treatment and preferred impact,"The ProPublica dataset includes data from the COMPAS risk assessment system. This dataset was previously extensively used for fairness analysis in the field of criminal justice risk. The dataset includes 6,167 individuals, and the features in the dataset include number of previous felonies, charge degree, age, race, and gender. The target variable indicates whether an inmate recidivated (was arrested again) within 2 years after release from prison. As for the sensitive variable, this dataset was previously used with two variations: the first when race was considered as the sensitive attribute and the second when gender was considered as the sensitive attribute. The dataset is publicly available from Larson et al.",In-process mechanisms that solves an optimization problem that balances accuracy and tractable proxies for preferred treatment and preferred impact.
16,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",ProPublica,"6,167",Criminal risk,yes,"Race, Gender/Sex",,In-process,Linear classifier,Pairwise fairness,Algorithmic Framework for Fairness Elicitation,"The ProPublica dataset includes data from the COMPAS risk assessment system. This dataset was previously extensively used for fairness analysis in the field of criminal justice risk. The dataset includes 6,167 individuals, and the features in the dataset include number of previous felonies, charge degree, age, race, and gender. The target variable indicates whether an inmate recidivated (was arrested again) within 2 years after release from prison. As for the sensitive variable, this dataset was previously used with two variations: the first when race was considered as the sensitive attribute and the second when gender was considered as the sensitive attribute. The dataset is publicly available from Larson et al.",In-process mechanism that leverages external information on equally deserving individuals. The optimization procedure trades off classification accuracy with frequency of decisions that violates human subjective fairness judgments.
16,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",ProPublica,"6,167",Criminal risk,yes,"Race, Gender/Sex",,In-process,SVM,"Equalized odds, Overall accuracy equality",Recycling Privileged Learning and Distribution Matching,"The ProPublica dataset includes data from the COMPAS risk assessment system. This dataset was previously extensively used for fairness analysis in the field of criminal justice risk. The dataset includes 6,167 individuals, and the features in the dataset include number of previous felonies, charge degree, age, race, and gender. The target variable indicates whether an inmate recidivated (was arrested again) within 2 years after release from prison. As for the sensitive variable, this dataset was previously used with two variations: the first when race was considered as the sensitive attribute and the second when gender was considered as the sensitive attribute. The dataset is publicly available from Larson et al.",Use SVM with privileged learning. Combine the sensitive attributes as privileged information that is known only at training time. Use the MMD criterion to encourage the distributions to be similar across privileged and unprivileged groups.
16,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",ProPublica,"6,167",Criminal risk,yes,"Race, Gender/Sex",,In-process,Any,"Demographic parity, Equalized odds",ReductionsApproach,"The ProPublica dataset includes data from the COMPAS risk assessment system. This dataset was previously extensively used for fairness analysis in the field of criminal justice risk. The dataset includes 6,167 individuals, and the features in the dataset include number of previous felonies, charge degree, age, race, and gender. The target variable indicates whether an inmate recidivated (was arrested again) within 2 years after release from prison. As for the sensitive variable, this dataset was previously used with two variations: the first when race was considered as the sensitive attribute and the second when gender was considered as the sensitive attribute. The dataset is publicly available from Larson et al.",A minimax optimization formulation that aims at maximizing the predictor’s capability to predict the outcome while minimizing the capability to predict the sensitive feature. Solved using saddle point methods.
16,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",ProPublica,"6,167",Criminal risk,yes,"Race, Gender/Sex",,Post-process,Any score based,"Demographic parity, Conditional statistical parity, Predictive parity",Cost of Fairness,"The ProPublica dataset includes data from the COMPAS risk assessment system. This dataset was previously extensively used for fairness analysis in the field of criminal justice risk. The dataset includes 6,167 individuals, and the features in the dataset include number of previous felonies, charge degree, age, race, and gender. The target variable indicates whether an inmate recidivated (was arrested again) within 2 years after release from prison. As for the sensitive variable, this dataset was previously used with two variations: the first when race was considered as the sensitive attribute and the second when gender was considered as the sensitive attribute. The dataset is publicly available from Larson et al.",Selecting separate thresholds for each group that maximizes accuracy while minimizing demographic parity.
16,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",ProPublica,"6,167",Criminal risk,yes,"Race, Gender/Sex",,Post-process,Any,Combination of individual and group fairness,Individual+Group Debiasing,"The ProPublica dataset includes data from the COMPAS risk assessment system. This dataset was previously extensively used for fairness analysis in the field of criminal justice risk. The dataset includes 6,167 individuals, and the features in the dataset include number of previous felonies, charge degree, age, race, and gender. The target variable indicates whether an inmate recidivated (was arrested again) within 2 years after release from prison. As for the sensitive variable, this dataset was previously used with two variations: the first when race was considered as the sensitive attribute and the second when gender was considered as the sensitive attribute. The dataset is publicly available from Larson et al.",Post-process mechanism to balance accuracy with both individual and group fairness. Begin by detecting samples that are prone to individual bias and consider them for prediction change for enhancing disparate impact. Note that this method does not require ground truth class labels for the validation set.
17,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Apnea,,,,,,Pre-process,Any,Group preference-based fairness,Decoupled Classifiers with Preference Guarantees,,Recursive feature selection procedure for building decoupled classifiers.
18,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",College Admissions,"20,000 admissions",,no,"Gender/Sex, Race",,In-process,Decision boundary based,Equalized odds,Penalizing Unfairness,"The College Admissions dataset was collected by the UCLA law school. It includes data from more than 20,000 records of law school students who took the bar exam. The goal of this dataset is to predict whether a student will pass the exam based on factors such as LSAT score, undergraduate GPA, and family income. This dataset was used, for example, by Berk et al., where gender was studied as the sensitive feature, and Bechavod and Ligett, where race was studied as the sensitive feature.",Minimizing a proxy for equalized odds in a penalty to the objective function.
18,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",College Admissions,"20,000 admissions",,no,"Gender/Sex, Race",,In-process,Any,"Demographic parity, Equalized odds",Reductions Approach,"The College Admissions dataset was collected by the UCLA law school. It includes data from more than 20,000 records of law school students who took the bar exam. The goal of this dataset is to predict whether a student will pass the exam based on factors such as LSAT score, undergraduate GPA, and family income. This dataset was used, for example, by Berk et al., where gender was studied as the sensitive feature, and Bechavod and Ligett, where race was studied as the sensitive feature.",A minimax optimization formulation. that aims at maximizing the predictor’s capability to predict the outcome while minimizing the capability to predict the sensitive feature. Solved using saddle point methods.
19,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Loans Default,"30,000 loans",Loans,yes,Gender/Sex,,In-process,Decision boundary based,Equalized odds,Penalizing Unfairness,"The Loans Default dataset includes 30,000 instances and 24 attributes of credit card users. It is publicly available in the UCI repository. The goal is to predict whether a customer will default on payments. The features include age, gender, marital status, past payments, credit limit, and education. This dataset was used, for example, by Bechavod and Ligett and Yeh and Lien, where gender was studied as the sensitive feature.",Minimizing a proxy for equalized odds in a penalty to the objective function.
20,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",NYPD SQF,,,,,,In-process,Logistic regression,Procedural fairness,Feature Selection for Procedurally Fair Learning,,"Perform fair feature selection to balance procedural fairness (that considers fair process rather than fair outcome) and accuracy, using constrained submodular optimization. Combines human judgments to assess procedural fairness."
20,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",NYPD SQF,,,,,,In-process,Convex boundary-based classifiers,Group preference-based fairness,Achieving preferred treatment and preferred impact,,In-process mechanisms that solves an optimization problem that balances accuracy and tractable proxies for preferred treatment and preferred impact.
21,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Cancer,,,,,,Pre-process,Any,Group preference-based fairness,Decoupled Classifiers with Preference Guarantees,,Recursive feature selection procedure for building decoupled classifiers.
22,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Heritage health,"147,473",Health,yes,Age,,"Pre-process, In-process",Logistic regression,Demographic parity,Learning Fair Representations,"The Heritage health dataset originated from a competition conducted by the United States as a competition to improve healthcare through early prediction. It includes data of 147,473 patients with 139 features. The goal of this dataset is to predict whether an individual will spend any days in the hospital during the next year. This dataset was studied, for example, in the work of Zemel et al., Louizos et al., and Tramer et al., where age was the sensitive feature. The dataset is publicly available from Heritage Health Prize Contest Data","Combining fair representation learning with an in-process model by a multi-objective loss function that requires representations to retain information of the original vectors, induce fair statistical parity, and maintain as high accuracy as possible."
22,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Heritage health,"147,473",Health,yes,Age,,"Pre-process, In-process",Any,"Demographic parity, Mean difference",Variational Fair Autoencoder,"The Heritage health dataset originated from a competition conducted by the United States as a competition to improve healthcare through early prediction. It includes data of 147,473 patients with 139 features. The goal of this dataset is to predict whether an individual will spend any days in the hospital during the next year. This dataset was studied, for example, in the work of Zemel et al., Louizos et al., and Tramer et al., where age was the sensitive feature. The dataset is publicly available from Heritage Health Prize Contest Data",Fair representation learning using a variational autoencoder. Additional penalty for controlling distribution matching between groups using MMD criterion.
23,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Ricci,118,Healthcare and social,yes,Race,,,,,,"description 1: The Ricci dataset includes the results of an exam administered to 118 individuals to determine which of them would receive a promotion. The dataset originated from a case that was brought to the U.S. Supreme Court. The goal of this dataset is to successfully predict whether an individual receives a promotion based on features that were tested in the exam, as well as the current position of each individual. The sensitive attribute in this dataset is race. The dataset is publicly available from Friedler et al.",
23,11. A survey on datasets for fairness-aware machine learning,"Tai Le Quy1, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, Eirini Ntoutsi",Ricci,118,Healthcare and social,yes,Race,,,,,,"description 2: The Ricci dataset was generated by the Ricci v.DeStefano case (Supreme Court of the United States, 2009), in whichthey investigated the results of a promotion exam within a fire department in November 2003 and December 2003. Although it is a relatively small dataset, it has been employed for fairness-aware classification tasks in many studies.",
24,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Mexican Poverty,183,Poverty,no,"Young and old families, Urban and rural areas",,,,,,"The Mexican poverty dataset includes poverty estimation for determining whether to match households with social programs. The data originated from a survey of 70,305 households in 2016. The target feature is poverty level, and there are 183 features. This dataset was studied, for example, in the work of Bakker et al. and Noriega-Campero. The authors studied two sensitive features: young and old families, as well as urban and rural areas.",
25,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Diabetes,"100,000 incidents",Health,yes,Race,,,,,,"description 1: The Diabetes dataset includes hospital data for the task of predicting whether a patient will be readmitted. It is publicly available in the UCI repository. The data contain approximately 100,000 instances and 235 attributes. This dataset was studied, for example, in the work of Edwards and Storkey, where race was the sensitive feature.",
25,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",Diabetes,"100,000 incidents",Health,yes,,,,,,,"description 2: : It is widely used in healthcare research. the key characteristics of this dataset are nine attributes capturing health-related measurements and demographic information (numerical and categorical). The predicting attribute in the Diabetes Dataset is the “Outcome” attribute, which represents the presence or absence of diabetes in the individual. This attribute serves as the target variable for classification tasks. ",
25,11. A survey on datasets for fairness-aware machine learning,"Tai Le Quy1, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, Eirini Ntoutsi",Diabetes,"100,000 incidents",Health,yes,Gender/Sex,,,,,,description 3: The diabetes dataset describes the clinical care at 130 US hospitals and integrated delivery networks from 1999 to 2008. The classification task is to predict whether a patient will readmit within 30 days.,
26,3. A Review on Fairness in Machine Learning,"DANA PESSACH, EREZ SHMUELI",Bank Marketing,"41,188",Financial,yes,Age,,,,,,"description 1: The Bank Marketing dataset is a publicly available dataset in the UCI repository, and it includes 41,188 individuals with 20 attributes. The task is to predict whether the client has subscribed to a term deposit service based on features such as marital status and age. It was previously investigated by Zafar et al., where age was studied as the sensitive attribute.",
26,11. A survey on datasets for fairness-aware machine learning,"Tai Le Quy1, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, Eirini Ntoutsi",Bank Marketing,"45,211",Financial,yes,"Age, Marital",,,,,,description 2: The bank marketing dataset is related to the direct marketing campaigns of a Portuguese banking institution from 2008 to 2013. There is a variety of researchers investigating this dataset in their studies. The classification goal is to predict whether a client will make a deposit subscription or not.,
27,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",AFHQ,,,,,,In-process,,"Demographic parity, Equalized odds",,,
28,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",Cats and Dogs,,,,,,In-process,,"Demographic parity, Equalized odds",,,
29,2. A Survey on Bias and Fairness in Machine Learning,"NINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA, KRISTINA LERMAN, ARAM GALSTYAN",COMPAS,"18,610 crime records",Criminology,,"Race, Gender/Sex",,,,,,"description 1: The COMPAS dataset contains records for defendants from Broward County indicating their jail and prison times, demographics, criminal histories, and COMPAS risk scores from 2013 to 2014.",
29,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",COMPAS,"18,610 crime records",Criminology,,"Race, Gender/Sex",,Pre-process,,"Demographic parity, Positive Predictive Value, FNR, FPR",,"description 2: The COMPAS dataset describes a binary classification task, which shows whether an inmate will re-offend within two years, has sensitive attributes such as race, age, and gender. This is one of the most widely used datasets for bias and fairness experiments, with a controversial and relevant topic.",
29,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",COMPAS,"18,610 crime records",Criminology,,"Race, Gender/Sex",,In-process,,"Disparate impact, FNR, TPR, FPR, Demographic parity, Equalized odds, Equality of Opportunity, Accuracy",,"description 2: The COMPAS dataset describes a binary classification task, which shows whether an inmate will re-offend within two years, has sensitive attributes such as race, age, and gender. This is one of the most widely used datasets for bias and fairness experiments, with a controversial and relevant topic.",
29,11. A survey on datasets for fairness-aware machine learning,"Tai Le Quy1, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, Eirini Ntoutsi",COMPAS,"18,610 crime records",Criminology,,"Race, Gender/Sex",,,,,,"description 3: The COMPAS is a recent dataset, compared to the rest of the datasets in our survey, which was released by ProPublica14 in 2016 based on the Broward County data.",
30,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",FlickrFaces,,,,,,In-process,,"Demographic parity, Equalized odds",,,
31,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",Student,,,,,,In-process,,"Demographic parity, Equalized odds",,,
32,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",MovieLens,"1,000,000",,,,,In-process,,"Statistical Parity, Equalized odds Difference, Equalized odds",,"The MovieLens 1M dataset contains a set of movie ratings from the MovieLens website, a movie recommendation service of 1 million reviews from 6000 users for 4000 movies, with demographics such as gender, age, occupation, and zip code, plus data from the movies and the ratings.",
33,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",Florida Department of Corrections (FDOC),,,,,,In-process,,"FPR, FNR, Accuracy",,"The FDOC dataset, on the other hand, contains sentences with the types of charges, which can be violent charges (murder, manslaughter, sex crimes, and other violent crimes); robbery; burglary; other property charges (including theft, fraud, and damage); drug-related charges; and other charges (including weapons and other public order offenses). The dataset uses Florida Department of Law Enforcement (FDLE) criminal history records for recidivism information within 3 years. They have the attributes such as the major crime category, the offender’s age of admission and release, time served in prison, number of crimes committed prior to arrest, race, marital status, employment status, gender, education level, and if recidivist whether they were supervised after release.",
34,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",Boston housing price,,,,,,Pre-process,,,,"The Boston dataset has data extracted from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970 and each of the 506 samples represents data obtained on 14 characteristics for households. The classification of this model aims to predict the property value of the region using attributes such as crime rate, proportion of residential land, and average number of rooms per household, among others.",
35,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",Medical Expenditure Panel Survey (MEPS),,,,,,In-process,,"Absolute Balanced Accuracy Difference, Absolute Average Odds Difference, Absolute Equal Opportunity Rate Difference, Statistical Parity Difference",,"The MEPS dataset contains data on families and individuals in the United States, with their medical providers and employers, with information on the cost and use of health care or insurance.",
35,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",Medical Expenditure Panel Survey (MEPS),,,,,,Pre-process,,,,"The MEPS dataset contains data on families and individuals in the United States, with their medical providers and employers, with information on the cost and use of health care or insurance.",
36,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",Heart Disease,,,,,,Pre-process,,,,"The MEPS dataset contains data on families and individuals in the United States, with their medical providers and employers, with information on the cost and use of health care or insurance.",
37,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",Multiparameter Intelligent Monitoring in Intensive Care MIMIC II,,,,"Gender/Sex, Age, Weight, Height, smoking, etc",,Pre-process,,,,"MIMIC II dataset contains vital signs captured from patient monitors and clinical data from tens of thousands of Intensive Care Unit (ICU) patients. It has demographic data such as patient gender and age, hospital admissions and discharge dates, room tracking, dates of death (in or out of hospital), ICD-9 codes, unique code for healthcare professional and patient type, as well as medications, lab tests, fluid administration, notes, and reports.",
38,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",Weight,,,,,,In-process,,"Equality of Opportunity, Demographic parity",,"The Weight dataset contains data for estimating obesity levels in individuals from the countries of Mexico, Peru, and Colombia, based on their eating habits and physical condition. It has 17 attributes and 2111 samples, labeled with the level of obesity which can be Low Weight, Normal Weight, Overweight Level I, Overweight Level II, Obesity Type I, Obesity Type II and Obesity Type III. The sensitive attributes are gender, age, weight, height, and smoking, among others.",
39,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",Drug,,,,,,In-process,,"Demographic parity, Equalized odds",,"To predict narcotic use, the Drug dataset was collected from an online survey including personality traits (NEO-FFI-R), impulsivity (BIS-11), sensation seeking (ImpSS), and demographic information. The dataset contains information on the use of 18 central nervous system psychoactive drugs such as amphetamines, cannabis, cocaine, ecstasy, legal drugs, LSD, and magic mushrooms, among others, including demographic attributes such as gender, education level, and age group.",
40,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",Labeled Faces in the Wild (LFW),,Facial images,,,,In-process,,"TPR, Equalized odds",,"The LFW dataset contains 13,233 images of faces of 5749 distinct people and1680 individuals are in two or more images. LFW is applied to face recognition problems and the images were annotated for demographic information such as gender, ethnicity,skin color, age group, hair color, eyeglass wearing, among other sensitive attributes",
41,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",Labeled Faces in the Wild (LFW),,Facial images,,,,In-process,,"TPR, Equalized odds",,"The LFW dataset contains 13,233 images of faces of 5749 distinct people and1680 individuals are in two or more images. LFW is applied to face recognition problems and the images were annotated for demographic information such as gender, ethnicity,skin color, age group, hair color, eyeglass wearing, among other sensitive attributes",
42,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento",Large-scale CelebFaces Attributes (CelebA),,Facial images,,,,In-process,,"TPR, Equalized odds, Demographic parity",,"The CelebA dataset contains 202,599 face images with 10,177 individuals and 40 annotated attributes per image such as gender, Asian features, skin color, age group, head color, and eye color, among other sensitive attributes, just as LFW is also used for face recognition problems",
43,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods","Tiago P. Pagano, Rafael B. Loureiro, Fernanda V. N. Lisboa, Rodrigo M. Peixoto, Guilherme A. S. Guimarães, Gustavo O. R. Cruz, Maira M. Araujo, Lucas L. Santos, Marco A. S. Cruz, Ewerton L. S. Oliveira, Ingrid Winkler, Erick G. S. Nascimento","MORPH Longitudinal Database",,Facial images,,,,,,,,"The MORPH dataset contains over 400,000 images of almost 70,000 individuals. The images are 8-bit color and sizes can vary. MORPH has annotations for age, sex, race, height, weight, and eye coordinates.",
44,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods",,Visual Question Answering (VQA),,,,,,Post-process,,"Accuracy, Precision, Recall, F1-score, ROC",,"The VQA dataset contains natural language questions about images. It has 250,000 images, 760,000 questions, and about 10 million answers. The questions have a sensitive criterion from the point of view of the questioner and can be a simple question or a difficult one, creating a bias. The images can also be very complex, making it difficult to identify the question element.",
45,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods",,VizWiz,,,,,,Post-process,,"Accuracy, Precision, Recall, F1-score, ROC",,"The VizWiz dataset has the same proposal as the VQA for object recognition and assistive technologies, collected from users with visual impairment.",
46,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods",,CLEVR,,,,,,Post-process,,"Accuracy, Precision, Recall, F1-score, ROC",,"CLEVR has a similar proposal to VQA and VizWiz but was generated automatically by algorithms containing images with three basic shapes (spheres, cubes, and cylinders) in two different sizes (small and large) and eight different colors and includes questions and answers with the elements contained in the images.",
47,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods",,Animal FacesHQ (AFHQ),,Facial images,,,,In-process,,"Demographic parity, Equalized odds",,"The dataset Animal FacesHQ (AFHQ) deals with the identification of animals, and the bias is associated with implicit features of the images. The AFHQ dataset consists of 15,000 high-quality images of animal faces at 512 × 512 resolution. It includes three domains of cat, dog, and wildlife, each providing 5000 images, it also contains three domains and several images of various breeds (larger than eight) for each domain. All images are aligned vertically and horizontally to have the eyes in the center. Low-quality images were discarded. The work by Jalal et al. used only images of cats and dogs.",
48,"8. Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods",,Correlated and Imbalanced MNIST (CI-MNIST),,,,,,In-process,,"Equalized odds, Equality of Opportunity, Demographic parity, Accuracy",,"The CI-MNIST dataset is a variant of the MNIST dataset with additional artificial attributes for eligibility analysis. For an image, the label indicates eligibility or ineligibility, respectively, given that it is even or odd. The dataset varies the background colors as a protected or sensitive attribute, where blue denotes the nonprivileged group and red denotes the privileged group. The dataset is designed to evaluate bias mitigation approaches in challenging situations and address different situations. The dataset has 50,000 images for the training set, 10,000 images for validation and testing, with the eligible images representing 50 percent of each of  hese. Various background colors, colored boxes added at some top of the image of varying sizes were used to allow the impact of the colors, positions, and sizes of the elements contained in the image to be analyzed.",
49,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",HELOC,,,,,,,,,,"The HELOC (Home Equity Line of Credit) dataset is a real-world dataset popularly known in the feld of credit risk assessment and lending. It is a real-world dataset commonly used in credit risk assessment and lending. It contains demographic information about borrowers’ credit profles, loan applications, and loan performance related to home equity lines of credit. Researchers often utilize this dataset for developing and evaluating machine learning models to predict credit risk, \determine the likelihood of defaulting on a loan, improve lending decisions, manage credit risk, and assess the fairness of loan approval processes.",
50,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",Child welfare,,,,,,,,,,"It ontains child welfare and protection information. The specifc attributes and labels in the dataset may vary depending on the source and purpose of the dataset. However, there are usually demographic attributes (age, gender, ethnicity), socioeconomic attributes (income, education level), case information (referral, abuse or neglect reported), family history (parental substance abuse, domestic violence, or mental health issues), placement history (foster placement history), service utilization (counseling, therapy, or parenting programs). In the academic feld, the Child Welfare dataset is a valuable resource for understanding the ethical dimensions of child welfare practices and the implications of using AI technologies in this domain. It allows researchers to identify potential challenges, propose solutions, and contribute to developing ethical guidelines and frameworks for AI systems in child welfare. ",
51,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",Pima Indian diabetes,,,,,,,,,,"The dataset serves as a benchmark for evaluating the performance of different classifcation algorithms in the context of diabetes prediction. Researchers can compare their models’ accuracy, sensitivity, specifcity, and other metrics with existing literature that utilizes this dataset",
52,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",Outbrain Click Prediction,,,yes,,,,,,,Available in Kaggle. Used to compare test scores using different positional approaches,
53,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",KKBox’s Music Recommendation Challenge,,,yes,,,,,,,Available in Kaggle. Used to compare test scores using different positional approaches,
54,"9. Fairness issues, current approaches, and challenges in machine learning models","Tonni Das Jui, Pablo Rivas",HMDA,,,,,,,,,,"The Home Mortgage Confict of interest Act (HMDA) dataset is a collection of data related to mortgage applications and loans in the United States. It contains information on various attributes related to loan applications, borrowers, lenders, and loan characteristics. The dataset provides valuable insights into lending practices and can be used to analyze mortgage market trends, identify potential disparities or biases, and assess fair lending practices. The attributes included in the HMDA dataset can vary depending on the year and jurisdiction. However, typical attributes in HMDA datasets include applicant’s information, load information, and lender information. The dataset also includes information on the loan approval status, denials, and other loan-related outcomes, which can be used as labels for predicting loan outcomes or assessing fairness.",
55,11. A survey on datasets for fairness-aware machine learning,"Tai Le Quy1, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, Eirini Ntoutsi",KDD Census-Income,"299,285",Financial,,"Gender/Sex, Race",,,,,,"The KDD Census-Income dataset was collected from Current Population Surveys implemented by the US Census Bureau from 1994 to 1995. The dataset has been considered in numerous related works. The prediction task is to decide if a person receives more than 50,000 US dollars annually or not.",
56,11. A survey on datasets for fairness-aware machine learning,"Tai Le Quy1, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, Eirini Ntoutsi",Credit card clients,"30,000 customers",Financial,,"Gender/Sex, Education",,,,,,The credit card clients13 dataset investigated the customers' default payments in Taiwan in October 2005. The goal is to predict whether a customer will face the default situation in the next month or not. The data have been used for default payment prediction in several studies.,
57,11. A survey on datasets for fairness-aware machine learning,"Tai Le Quy1, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, Eirini Ntoutsi",Student performance,434,Educational,,"Gender/Sex, Age, (Romantic, Dalc, Walc)",,,,,,The student performance dataset described students' achievement in the secondary education of two Portuguese schools in 2005–2006 with two distinct subjects: Mathematics and Portuguese. The regression task is to predict the final year grade of the students. It is investigated in several researches with fairness-aware regression and clustering approaches.,
58,11. A survey on datasets for fairness-aware machine learning,"Tai Le Quy1, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, Eirini Ntoutsi",Open University Learning Analytics,"32,593",Educational,,Gender/Sex,,,,,,"The Open University Learning Analytics (OULAD) dataset was collected from the OU analysis project of The Open University in 2013–2014. The dataset contains information of students and their activities in the virtual learning environment (VLE) for seven courses. The dataset is investigated in several papers, on fairness-aware problems. The goal is to predict the success of students.",
59,11. A survey on datasets for fairness-aware machine learning,"Tai Le Quy1, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, Eirini Ntoutsi",Law school,"20,798",Educational,,"Gender/Sex, Race",,,,,,The Law school dataset was conducted by a Law School Admission Council (LSAC) survey across 163 law schools in the United States in 1991. The dataset contains the law school admission records. The prediction task is to predict whether a candidate would pass the bar exam or predict a student's first-year average grade.,